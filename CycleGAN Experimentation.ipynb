{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmw/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'cyclegan_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-20f4b773d3dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcyclegan_datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'cyclegan_datasets'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from scipy.misc import imsave\n",
    "\n",
    "import click\n",
    "import tensorflow as tf\n",
    "\n",
    "from . import cyclegan_datasets\n",
    "from . import data_loader, losses, model\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "\n",
    "class CycleGAN:\n",
    "    \"\"\"The CycleGAN module.\"\"\"\n",
    "\n",
    "    def __init__(self, pool_size, lambda_a,\n",
    "                 lambda_b, output_root_dir, to_restore,\n",
    "                 base_lr, max_step, network_version,\n",
    "                 dataset_name, checkpoint_dir, do_flipping, skip):\n",
    "        current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "        self._pool_size = pool_size\n",
    "        self._size_before_crop = 286\n",
    "        self._lambda_a = lambda_a\n",
    "        self._lambda_b = lambda_b\n",
    "        self._output_dir = os.path.join(output_root_dir, current_time)\n",
    "        self._images_dir = os.path.join(self._output_dir, 'imgs')\n",
    "        self._num_imgs_to_save = 20\n",
    "        self._to_restore = to_restore\n",
    "        self._base_lr = base_lr\n",
    "        self._max_step = max_step\n",
    "        self._network_version = network_version\n",
    "        self._dataset_name = dataset_name\n",
    "        self._checkpoint_dir = checkpoint_dir\n",
    "        self._do_flipping = do_flipping\n",
    "        self._skip = skip\n",
    "\n",
    "        self.fake_images_A = np.zeros(\n",
    "            (self._pool_size, 1, model.IMG_HEIGHT, model.IMG_WIDTH,\n",
    "             model.IMG_CHANNELS)\n",
    "        )\n",
    "        self.fake_images_B = np.zeros(\n",
    "            (self._pool_size, 1, model.IMG_HEIGHT, model.IMG_WIDTH,\n",
    "             model.IMG_CHANNELS)\n",
    "        )\n",
    "\n",
    "    def model_setup(self):\n",
    "        \"\"\"\n",
    "        This function sets up the model to train.\n",
    "        self.input_A/self.input_B -> Set of training images.\n",
    "        self.fake_A/self.fake_B -> Generated images by corresponding generator\n",
    "        of input_A and input_B\n",
    "        self.lr -> Learning rate variable\n",
    "        self.cyc_A/ self.cyc_B -> Images generated after feeding\n",
    "        self.fake_A/self.fake_B to corresponding generator.\n",
    "        This is use to calculate cyclic loss\n",
    "        \"\"\"\n",
    "        self.input_a = tf.placeholder(\n",
    "            tf.float32, [\n",
    "                1,\n",
    "                model.IMG_WIDTH,\n",
    "                model.IMG_HEIGHT,\n",
    "                model.IMG_CHANNELS\n",
    "            ], name=\"input_A\")\n",
    "        self.input_b = tf.placeholder(\n",
    "            tf.float32, [\n",
    "                1,\n",
    "                model.IMG_WIDTH,\n",
    "                model.IMG_HEIGHT,\n",
    "                model.IMG_CHANNELS\n",
    "            ], name=\"input_B\")\n",
    "\n",
    "        self.fake_pool_A = tf.placeholder(\n",
    "            tf.float32, [\n",
    "                None,\n",
    "                model.IMG_WIDTH,\n",
    "                model.IMG_HEIGHT,\n",
    "                model.IMG_CHANNELS\n",
    "            ], name=\"fake_pool_A\")\n",
    "        self.fake_pool_B = tf.placeholder(\n",
    "            tf.float32, [\n",
    "                None,\n",
    "                model.IMG_WIDTH,\n",
    "                model.IMG_HEIGHT,\n",
    "                model.IMG_CHANNELS\n",
    "            ], name=\"fake_pool_B\")\n",
    "\n",
    "        self.global_step = slim.get_or_create_global_step()\n",
    "\n",
    "        self.num_fake_inputs = 0\n",
    "\n",
    "        self.learning_rate = tf.placeholder(tf.float32, shape=[], name=\"lr\")\n",
    "\n",
    "        inputs = {\n",
    "            'images_a': self.input_a,\n",
    "            'images_b': self.input_b,\n",
    "            'fake_pool_a': self.fake_pool_A,\n",
    "            'fake_pool_b': self.fake_pool_B,\n",
    "        }\n",
    "\n",
    "        outputs = model.get_outputs(\n",
    "            inputs, network=self._network_version, skip=self._skip)\n",
    "\n",
    "        self.prob_real_a_is_real = outputs['prob_real_a_is_real']\n",
    "        self.prob_real_b_is_real = outputs['prob_real_b_is_real']\n",
    "        self.fake_images_a = outputs['fake_images_a']\n",
    "        self.fake_images_b = outputs['fake_images_b']\n",
    "        self.prob_fake_a_is_real = outputs['prob_fake_a_is_real']\n",
    "        self.prob_fake_b_is_real = outputs['prob_fake_b_is_real']\n",
    "\n",
    "        self.cycle_images_a = outputs['cycle_images_a']\n",
    "        self.cycle_images_b = outputs['cycle_images_b']\n",
    "\n",
    "        self.prob_fake_pool_a_is_real = outputs['prob_fake_pool_a_is_real']\n",
    "        self.prob_fake_pool_b_is_real = outputs['prob_fake_pool_b_is_real']\n",
    "\n",
    "    def compute_losses(self):\n",
    "        \"\"\"\n",
    "        In this function we are defining the variables for loss calculations\n",
    "        and training model.\n",
    "        d_loss_A/d_loss_B -> loss for discriminator A/B\n",
    "        g_loss_A/g_loss_B -> loss for generator A/B\n",
    "        *_trainer -> Various trainer for above loss functions\n",
    "        *_summ -> Summary variables for above loss functions\n",
    "        \"\"\"\n",
    "        cycle_consistency_loss_a = \\\n",
    "            self._lambda_a * losses.cycle_consistency_loss(\n",
    "                real_images=self.input_a, generated_images=self.cycle_images_a,\n",
    "            )\n",
    "        cycle_consistency_loss_b = \\\n",
    "            self._lambda_b * losses.cycle_consistency_loss(\n",
    "                real_images=self.input_b, generated_images=self.cycle_images_b,\n",
    "            )\n",
    "\n",
    "        lsgan_loss_a = losses.lsgan_loss_generator(self.prob_fake_a_is_real)\n",
    "        lsgan_loss_b = losses.lsgan_loss_generator(self.prob_fake_b_is_real)\n",
    "\n",
    "        g_loss_A = \\\n",
    "            cycle_consistency_loss_a + cycle_consistency_loss_b + lsgan_loss_b\n",
    "        g_loss_B = \\\n",
    "            cycle_consistency_loss_b + cycle_consistency_loss_a + lsgan_loss_a\n",
    "\n",
    "        d_loss_A = losses.lsgan_loss_discriminator(\n",
    "            prob_real_is_real=self.prob_real_a_is_real,\n",
    "            prob_fake_is_real=self.prob_fake_pool_a_is_real,\n",
    "        )\n",
    "        d_loss_B = losses.lsgan_loss_discriminator(\n",
    "            prob_real_is_real=self.prob_real_b_is_real,\n",
    "            prob_fake_is_real=self.prob_fake_pool_b_is_real,\n",
    "        )\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate, beta1=0.5)\n",
    "\n",
    "        self.model_vars = tf.trainable_variables()\n",
    "\n",
    "        d_A_vars = [var for var in self.model_vars if 'd_A' in var.name]\n",
    "        g_A_vars = [var for var in self.model_vars if 'g_A' in var.name]\n",
    "        d_B_vars = [var for var in self.model_vars if 'd_B' in var.name]\n",
    "        g_B_vars = [var for var in self.model_vars if 'g_B' in var.name]\n",
    "\n",
    "        self.d_A_trainer = optimizer.minimize(d_loss_A, var_list=d_A_vars)\n",
    "        self.d_B_trainer = optimizer.minimize(d_loss_B, var_list=d_B_vars)\n",
    "        self.g_A_trainer = optimizer.minimize(g_loss_A, var_list=g_A_vars)\n",
    "        self.g_B_trainer = optimizer.minimize(g_loss_B, var_list=g_B_vars)\n",
    "\n",
    "        for var in self.model_vars:\n",
    "            print(var.name)\n",
    "\n",
    "        # Summary variables for tensorboard\n",
    "        self.g_A_loss_summ = tf.summary.scalar(\"g_A_loss\", g_loss_A)\n",
    "        self.g_B_loss_summ = tf.summary.scalar(\"g_B_loss\", g_loss_B)\n",
    "        self.d_A_loss_summ = tf.summary.scalar(\"d_A_loss\", d_loss_A)\n",
    "        self.d_B_loss_summ = tf.summary.scalar(\"d_B_loss\", d_loss_B)\n",
    "\n",
    "    def save_images(self, sess, epoch):\n",
    "        \"\"\"\n",
    "        Saves input and output images.\n",
    "        :param sess: The session.\n",
    "        :param epoch: Currnt epoch.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self._images_dir):\n",
    "            os.makedirs(self._images_dir)\n",
    "\n",
    "        names = ['inputA_', 'inputB_', 'fakeA_',\n",
    "                 'fakeB_', 'cycA_', 'cycB_']\n",
    "\n",
    "        with open(os.path.join(\n",
    "                self._output_dir, 'epoch_' + str(epoch) + '.html'\n",
    "        ), 'w') as v_html:\n",
    "            for i in range(0, self._num_imgs_to_save):\n",
    "                print(\"Saving image {}/{}\".format(i, self._num_imgs_to_save))\n",
    "                inputs = sess.run(self.inputs)\n",
    "                fake_A_temp, fake_B_temp, cyc_A_temp, cyc_B_temp = sess.run([\n",
    "                    self.fake_images_a,\n",
    "                    self.fake_images_b,\n",
    "                    self.cycle_images_a,\n",
    "                    self.cycle_images_b\n",
    "                ], feed_dict={\n",
    "                    self.input_a: inputs['images_i'],\n",
    "                    self.input_b: inputs['images_j']\n",
    "                })\n",
    "\n",
    "                tensors = [inputs['images_i'], inputs['images_j'],\n",
    "                           fake_B_temp, fake_A_temp, cyc_A_temp, cyc_B_temp]\n",
    "\n",
    "                for name, tensor in zip(names, tensors):\n",
    "                    image_name = name + str(epoch) + \"_\" + str(i) + \".jpg\"\n",
    "                    imsave(os.path.join(self._images_dir, image_name),\n",
    "                           ((tensor[0] + 1) * 127.5).astype(np.uint8)\n",
    "                           )\n",
    "                    v_html.write(\n",
    "                        \"<img src=\\\"\" +\n",
    "                        os.path.join('imgs', image_name) + \"\\\">\"\n",
    "                    )\n",
    "                v_html.write(\"<br>\")\n",
    "\n",
    "    def fake_image_pool(self, num_fakes, fake, fake_pool):\n",
    "        \"\"\"\n",
    "        This function saves the generated image to corresponding\n",
    "        pool of images.\n",
    "        It keeps on feeling the pool till it is full and then randomly\n",
    "        selects an already stored image and replace it with new one.\n",
    "        \"\"\"\n",
    "        if num_fakes < self._pool_size:\n",
    "            fake_pool[num_fakes] = fake\n",
    "            return fake\n",
    "        else:\n",
    "            p = random.random()\n",
    "            if p > 0.5:\n",
    "                random_id = random.randint(0, self._pool_size - 1)\n",
    "                temp = fake_pool[random_id]\n",
    "                fake_pool[random_id] = fake\n",
    "                return temp\n",
    "            else:\n",
    "                return fake\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Training Function.\"\"\"\n",
    "        # Load Dataset from the dataset folder\n",
    "        self.inputs = data_loader.load_data(\n",
    "            self._dataset_name, self._size_before_crop,\n",
    "            True, self._do_flipping)\n",
    "\n",
    "        # Build the network\n",
    "        self.model_setup()\n",
    "\n",
    "        # Loss function calculations\n",
    "        self.compute_losses()\n",
    "\n",
    "        # Initializing the global variables\n",
    "        init = (tf.global_variables_initializer(),\n",
    "                tf.local_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        max_images = cyclegan_datasets.DATASET_TO_SIZES[self._dataset_name]\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "\n",
    "            # Restore the model to run the model from last checkpoint\n",
    "            if self._to_restore:\n",
    "                chkpt_fname = tf.train.latest_checkpoint(self._checkpoint_dir)\n",
    "                saver.restore(sess, chkpt_fname)\n",
    "\n",
    "            writer = tf.summary.FileWriter(self._output_dir)\n",
    "\n",
    "            if not os.path.exists(self._output_dir):\n",
    "                os.makedirs(self._output_dir)\n",
    "\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "            # Training Loop\n",
    "            for epoch in range(sess.run(self.global_step), self._max_step):\n",
    "                print(\"In the epoch \", epoch)\n",
    "                saver.save(sess, os.path.join(\n",
    "                    self._output_dir, \"cyclegan\"), global_step=epoch)\n",
    "\n",
    "                # Dealing with the learning rate as per the epoch number\n",
    "                if epoch < 100:\n",
    "                    curr_lr = self._base_lr\n",
    "                else:\n",
    "                    curr_lr = self._base_lr - \\\n",
    "                        self._base_lr * (epoch - 100) / 100\n",
    "\n",
    "                self.save_images(sess, epoch)\n",
    "\n",
    "                for i in range(0, max_images):\n",
    "                    print(\"Processing batch {}/{}\".format(i, max_images))\n",
    "\n",
    "                    inputs = sess.run(self.inputs)\n",
    "\n",
    "                    # Optimizing the G_A network\n",
    "                    _, fake_B_temp, summary_str = sess.run(\n",
    "                        [self.g_A_trainer,\n",
    "                         self.fake_images_b,\n",
    "                         self.g_A_loss_summ],\n",
    "                        feed_dict={\n",
    "                            self.input_a:\n",
    "                                inputs['images_i'],\n",
    "                            self.input_b:\n",
    "                                inputs['images_j'],\n",
    "                            self.learning_rate: curr_lr\n",
    "                        }\n",
    "                    )\n",
    "                    writer.add_summary(summary_str, epoch * max_images + i)\n",
    "\n",
    "                    fake_B_temp1 = self.fake_image_pool(\n",
    "                        self.num_fake_inputs, fake_B_temp, self.fake_images_B)\n",
    "\n",
    "                    # Optimizing the D_B network\n",
    "                    _, summary_str = sess.run(\n",
    "                        [self.d_B_trainer, self.d_B_loss_summ],\n",
    "                        feed_dict={\n",
    "                            self.input_a:\n",
    "                                inputs['images_i'],\n",
    "                            self.input_b:\n",
    "                                inputs['images_j'],\n",
    "                            self.learning_rate: curr_lr,\n",
    "                            self.fake_pool_B: fake_B_temp1\n",
    "                        }\n",
    "                    )\n",
    "                    writer.add_summary(summary_str, epoch * max_images + i)\n",
    "\n",
    "                    # Optimizing the G_B network\n",
    "                    _, fake_A_temp, summary_str = sess.run(\n",
    "                        [self.g_B_trainer,\n",
    "                         self.fake_images_a,\n",
    "                         self.g_B_loss_summ],\n",
    "                        feed_dict={\n",
    "                            self.input_a:\n",
    "                                inputs['images_i'],\n",
    "                            self.input_b:\n",
    "                                inputs['images_j'],\n",
    "                            self.learning_rate: curr_lr\n",
    "                        }\n",
    "                    )\n",
    "                    writer.add_summary(summary_str, epoch * max_images + i)\n",
    "\n",
    "                    fake_A_temp1 = self.fake_image_pool(\n",
    "                        self.num_fake_inputs, fake_A_temp, self.fake_images_A)\n",
    "\n",
    "                    # Optimizing the D_A network\n",
    "                    _, summary_str = sess.run(\n",
    "                        [self.d_A_trainer, self.d_A_loss_summ],\n",
    "                        feed_dict={\n",
    "                            self.input_a:\n",
    "                                inputs['images_i'],\n",
    "                            self.input_b:\n",
    "                                inputs['images_j'],\n",
    "                            self.learning_rate: curr_lr,\n",
    "                            self.fake_pool_A: fake_A_temp1\n",
    "                        }\n",
    "                    )\n",
    "                    writer.add_summary(summary_str, epoch * max_images + i)\n",
    "\n",
    "                    writer.flush()\n",
    "                    self.num_fake_inputs += 1\n",
    "\n",
    "                sess.run(tf.assign(self.global_step, epoch + 1))\n",
    "\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "            writer.add_graph(sess.graph)\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"Test Function.\"\"\"\n",
    "        print(\"Testing the results\")\n",
    "\n",
    "        self.inputs = data_loader.load_data(\n",
    "            self._dataset_name, self._size_before_crop,\n",
    "            False, self._do_flipping)\n",
    "\n",
    "        self.model_setup()\n",
    "        saver = tf.train.Saver()\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "\n",
    "            chkpt_fname = tf.train.latest_checkpoint(self._checkpoint_dir)\n",
    "            saver.restore(sess, chkpt_fname)\n",
    "\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "            self._num_imgs_to_save = cyclegan_datasets.DATASET_TO_SIZES[\n",
    "                self._dataset_name]\n",
    "            self.save_images(sess, 0)\n",
    "\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "\n",
    "\n",
    "@click.command()\n",
    "@click.option('--to_train',\n",
    "              type=click.INT,\n",
    "              default=True,\n",
    "              help='Whether it is train or false.')\n",
    "@click.option('--log_dir',\n",
    "              type=click.STRING,\n",
    "              default=None,\n",
    "              help='Where the data is logged to.')\n",
    "@click.option('--config_filename',\n",
    "              type=click.STRING,\n",
    "              default='train',\n",
    "              help='The name of the configuration file.')\n",
    "@click.option('--checkpoint_dir',\n",
    "              type=click.STRING,\n",
    "              default='',\n",
    "              help='The name of the train/test split.')\n",
    "@click.option('--skip',\n",
    "              type=click.BOOL,\n",
    "              default=False,\n",
    "              help='Whether to add skip connection between input and output.')\n",
    "def main(to_train, log_dir, config_filename, checkpoint_dir, skip):\n",
    "    \"\"\"\n",
    "    :param to_train: Specify whether it is training or testing. 1: training; 2:\n",
    "     resuming from latest checkpoint; 0: testing.\n",
    "    :param log_dir: The root dir to save checkpoints and imgs. The actual dir\n",
    "    is the root dir appended by the folder with the name timestamp.\n",
    "    :param config_filename: The configuration file.\n",
    "    :param checkpoint_dir: The directory that saves the latest checkpoint. It\n",
    "    only takes effect when to_train == 2.\n",
    "    :param skip: A boolean indicating whether to add skip connection between\n",
    "    input and output.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    with open(config_filename) as config_file:\n",
    "        config = json.load(config_file)\n",
    "\n",
    "    lambda_a = float(config['_LAMBDA_A']) if '_LAMBDA_A' in config else 10.0\n",
    "    lambda_b = float(config['_LAMBDA_B']) if '_LAMBDA_B' in config else 10.0\n",
    "    pool_size = int(config['pool_size']) if 'pool_size' in config else 50\n",
    "\n",
    "    to_restore = (to_train == 2)\n",
    "    base_lr = float(config['base_lr']) if 'base_lr' in config else 0.0002\n",
    "    max_step = int(config['max_step']) if 'max_step' in config else 200\n",
    "    network_version = str(config['network_version'])\n",
    "    dataset_name = str(config['dataset_name'])\n",
    "    do_flipping = bool(config['do_flipping'])\n",
    "\n",
    "    cyclegan_model = CycleGAN(pool_size, lambda_a, lambda_b, log_dir,\n",
    "                              to_restore, base_lr, max_step, network_version,\n",
    "                              dataset_name, checkpoint_dir, do_flipping, skip)\n",
    "\n",
    "    if to_train > 0:\n",
    "        cyclegan_model.train()\n",
    "    else:\n",
    "        cyclegan_model.test()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
